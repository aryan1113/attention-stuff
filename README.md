RNN's by nature suffer from vanishing gradient and lack of zarallelization capability. Take a look into transformers and how they overcome these issues through some pretty neat tricks.

Here are some blogs 
Read more about position encodings [here : Medium Blog](https://medium.com/@aryan1113/positional-encoding-in-transformers-a1f24a7aa382) 

Read all about transformers [here : Medium Blog](https://medium.com/@aryan1113/transformers-overall-02f0f1a9e872)